<!DOCTYPE html>
<html lang="en">

<head>

  <!-- Minima -->
  <!-- Hexo theme created by @adisaktijrs -->

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">

  
  <title>Compression Works for Sparse Model Training</title>
  
  <link rel="sitemap" href="https://Fangjin98.github.iositemap.xml" />
  
  <link rel="canonical" href="https://fangjin98.github.io/2024/03/26/dt-compression/">
  
  <meta name="description" content="模型稀疏化是未来大模型发展的趋势，其中包括：  激活稀疏性：Attention 编码呈对焦聚焦，特殊token与邻近token的注意力得分比较高 权重稀疏性：Linear 层权重矩阵中较少比例的 outliers 或敏感值，对推理精度起重要作用 梯度稀疏性：包含了大量 embedding 层权重，具">
  
  
  <meta name="keywords" content="CS, Computer Networks, Powerlifting">
  
  <meta name="author" content="Joey">
  
  <meta property="og:image" content="https://Fangjin98.github.ioundefined">
  
  <meta property="og:site_name" content="Joey&#39;s Blog" />
  <meta property="og:type" content="article" />
  <meta property="og:title" content="Compression Works for Sparse Model Training" />
  
  <meta property="og:description" content="模型稀疏化是未来大模型发展的趋势，其中包括：  激活稀疏性：Attention 编码呈对焦聚焦，特殊token与邻近token的注意力得分比较高 权重稀疏性：Linear 层权重矩阵中较少比例的 outliers 或敏感值，对推理精度起重要作用 梯度稀疏性：包含了大量 embedding 层权重，具">
  
  <meta property="og:url" content="https://fangjin98.github.io/2024/03/26/dt-compression/" />

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Compression Works for Sparse Model Training">
  
  <meta name="twitter:description" content="模型稀疏化是未来大模型发展的趋势，其中包括：  激活稀疏性：Attention 编码呈对焦聚焦，特殊token与邻近token的注意力得分比较高 权重稀疏性：Linear 层权重矩阵中较少比例的 outliers 或敏感值，对推理精度起重要作用 梯度稀疏性：包含了大量 embedding 层权重，具">
  
  
  <meta name="twitter:image" content="https://Fangjin98.github.ioundefined">
  
  <meta name="twitter:url" content="https://fangjin98.github.io/2024/03/26/dt-compression/" />

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Preload fonts
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="preload" href="/fonts/dm-serif-display-v4-latin-regular.woff2" as="font" type="font/woff2" crossorigin>
  <link rel="preload" href="/fonts/inter-v2-latin-regular.woff2" as="font" type="font/woff2" crossorigin>

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  
<link rel="stylesheet" href="/css/normalize.css">

  
<link rel="stylesheet" href="/css/skeleton.css">

  
<link rel="stylesheet" href="/css/custom.css">

  
<link rel="stylesheet" href="/css/prism-dark.css">

  
<link rel="stylesheet" href="/css/prism-line-numbers.css">

  <!-- User css -->
  
  
<link rel="stylesheet" href="/css/user.css">

  

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="/images/favicon.png">

  <!-- Custom Theme Color Style
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <style>
  a:not(.icon) {
    text-decoration-color: #0FA0CE;
    background-image: linear-gradient(
      to bottom,
      rgba(0, 0, 0, 0) 50%,
      #0FA0CE 50%
    );
  }
  blockquote {
    border-left: 8px solid #0FA0CE;
  }
  .nanobar .bar {
    background: #0FA0CE;
  }
  .button.button-primary:hover,
  button.button-primary:hover,
  input[type="submit"].button-primary:hover,
  input[type="reset"].button-primary:hover,
  input[type="button"].button-primary:hover,
  .button.button-primary:focus,
  button.button-primary:focus,
  input[type="submit"].button-primary:focus,
  input[type="reset"].button-primary:focus,
  input[type="button"].button-primary:focus {
    background-color: #0FA0CE;
    border-color: #0FA0CE;
  }
  input[type="email"]:focus,
  input[type="number"]:focus,
  input[type="search"]:focus,
  input[type="text"]:focus,
  input[type="tel"]:focus,
  input[type="url"]:focus,
  input[type="password"]:focus,
  textarea:focus,
  select:focus {
    border: 1px solid #0FA0CE;
  }
</style>

  <!-- Google Analytics (With Privacy Settings On)
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  

<meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Joey's Blog" type="application/atom+xml">
</head>

<body>
  <div class="container">
    <div class="row">
      <div>
        <div class="row">
  <div class="two columns" style="max-width: 50px">
    <h1 class="mt-2 mode">
      <div onclick=setDarkMode(true) id="darkBtn">☀️</div>
      <div onclick=setDarkMode(false) id="lightBtn" class=hidden>🌑</div>
      <script >
        if (localStorage.getItem('preferredTheme') == 'dark') {
          setDarkMode(true)
        }
        function setDarkMode(isDark) {
          var darkBtn = document.getElementById('darkBtn')
          var lightBtn = document.getElementById('lightBtn')
          if (isDark) {
            lightBtn.style.display = "block"
            darkBtn.style.display = "none"
            localStorage.setItem('preferredTheme', 'dark');
          } else {
            lightBtn.style.display = "none"
            darkBtn.style.display = "block"
            localStorage.removeItem('preferredTheme');
          }
          document.body.classList.toggle("darkmode");
        }
      </script>
    </h1>
  </div>

  <div class="six columns ml-1">
    <h1 class="mt-2">
      Hello World.
    </h1>
  </div>

  <div class="twelve columns">
    <div class="row">
      <div class="nine columns left">
        <a href="/">Home</a>
        
          
          <a href="/about" class="ml">About</a>
          
        
          
          <a href="/archives" class="ml">Posts</a>
          
        
          
          <a href="/pdf/resume.pdf" class="ml">CV</a>
          
        
        
          
            <a href="mailto:Fangjin98@outlook.com" target="_blank" class="ml">Email</a>
          
        
      </div>
    </div>
    <hr style="margin-bottom: 2.6rem">
  </div>
</div>

        <div class="trans">
            <h2>Compression Works for Sparse Model Training</h2>

  <p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/kpuxrDg2qpVNY2JsHFzbyQ">模型稀疏化是未来大模型发展的趋势</a>，其中包括：</p>
<ul>
<li>激活稀疏性：Attention 编码呈对焦聚焦，特殊token与邻近token的注意力得分比较高</li>
<li>权重稀疏性：Linear 层权重矩阵中较少比例的 outliers 或敏感值，对推理精度起重要作用</li>
<li>梯度稀疏性：包含了大量 embedding 层权重，具有非零数值通常集中在部分区域（<strong>只有对应 batch 部分有非零值，其他部分为零</strong>）。</li>
</ul>
<p>为了克服稀疏化数据的影响，现有工作提出了权重压缩和梯度压缩工作。<br>
现有工作主要集中在软件层面研究如何进行梯度压缩（例如参数调优，模型架构调优），但是会导致端侧工作量增加，从而抵消通信量/参数量减少带来的好处。</p>
<h2 id="moe-model distributed training">MoE Model Distributed Training</h2>
<h3 id="model-architecture">Model Architecture</h3>
<p>传统 Transformer 模型层包含 Multihead Self-Attention (MSA) 和 Feed Forward Network (FFN) 模块，MoE 模块替代传统 Transformer 中的 FFN 模块，包含 1 个 Router 和多个 FFN，其中的一个 FFN 被称为一个专家。</p>
<p>多个 Transformer 层组成一个大模型。根据 Transformer 层的功能可以区分成 Encoder 和 Decoder。<br>
Encoder 通常为 MSA+FFN/MoE，Decoder 通常为  MSA+MLP(全联接层)。</p>
<h3 id="expert-parallel">Expert Parallel</h3>
<p>随着专家数变多，专家会分散在不同设备上，因此提出专家并行(Expert Parallel, EP)。<br>
Router 根据策略将每张卡的输入 token 路由到不同专家上。<br>
例如 Top-k 策略将 token 分配给概率最高的 k 个专家。</p>
<p>专家并行需要在 EP 域不同的卡之间引入 All-to-All 通信，收集每个专家负责的token。根据收集和分配一共需要 2 次 All-to-All。<br>
由于 All-to-All 通信需求均匀且多对多通信，每个专家具有相同的 capacity，多余的 token 会被丢弃，造成模型精度下降。</p>
<blockquote>
<p>通信域： Router 在每一路 DP 上存在相同的副本，不同 DP 下同一 EP 域内的卡间通信。</p>
</blockquote>
<h3 id="sequence-parallel">Sequence Parallel</h3>
<p>单机无法处理超长输入序列，因此划分多个子序列给不同设备处理，称为序列并行 (Sequence Parallel, SP)。<br>
SP 将 Layer Normalization 和 Dropout 进一步划分给不同设备上，相当于每个卡负责部分 Sequence 训练参数。<br>
引入 Ring Self-Attention 模组代替原先 Self-Attention 模组，训练过程中需要交换各自的Query、Key、Value embeddings（All-to-All通信）。</p>
<blockquote>
<p>通信内容：</p>
<ul>
<li>前向传递：Attention 部分需要2次 ring-P2P，MLP部分被TP切割，需要新增Linear1前1次 All-gather，Linear2后1次 Reduce scatter。</li>
<li>反向传递：MLP部分1次 allgather和1次reduce scatter。Attention可以复用1次allreduce通信。</li>
</ul>
</blockquote>
<h3 id="related-papers">Related Papers</h3>
<p>一些其他大模型训练相关的经典论文：</p>
<ul>
<li>ZeRO++: Extremely Efficient Collective Communication for Giant Model Training</li>
<li>DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2110.14883.pdf">Colossal-AI A Unified Deep Learning System For Large-Scale Parallel Training</a></li>
<li>Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</li>
<li>GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism</li>
</ul>
<h2 id="existing-work">Existing Work</h2>
<h3 id="model-(weight matrix) compression">Model (Weight Matrix) Compression</h3>
<blockquote>
<p>权重矩阵压缩多用于优化端侧内存读写，提高 cache 命中率，增加计算和访存的吞吐率。<br>
MoE 模型训练引入了大量权重矩阵、Attention层的通信内容，这类稀疏通信内容如何优化还未被讨论。</p>
</blockquote>
<p>针对稀疏权重压缩主要有结构化、半结构与非结构实现方式。</p>
<ol>
<li>结构化稀疏剪枝，粗粒度规整剪枝，压缩率提升，难以保证模型精度</li>
<li>半结构化稀疏，粒度较细，压缩率与精度受规整性影响</li>
<li>非结构稀疏化，粒度最细，高稀疏度下精度效果最好，但是压缩与加速收益取决于软硬件实现方法</li>
</ol>
<h3 id="gradient-compression">Gradient Compression</h3>
<p>在分布式训练过程中，梯度通常用于数据并行阶段，不同节点进行全局更新的聚合操作。<br>
现有工作聚焦如何设计通信、压缩策略，让稀疏梯度能够被高效聚合。</p>
<ul>
<li>Gradient Compression Supercharged High-Performance Data Parallel DNN Training</li>
<li><a target="_blank" rel="noopener" href="https://conferences.sigcomm.org/sigcomm/2021/files/papers/3452296.3472904.pdf">Efficient Sparse Collective Communication and its Application to Accelerate Distributed Deep Learning</a>：设计流式算法，包含协议设计，将梯度划分为block，按block传输数据，只传输非零数据</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2402.07529.pdf">Accelerating Distributed Deep Learning using Lossless Homomorphic Compression</a>：同态压缩，纯算法工作，设计机器学习压缩算法，支持在交换机上直接聚合</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2302.08545.pdf">THC: Accelerating Distributed Deep Learning Using Tensor Homomorphic Compression</a>：结合同态压缩设计网内聚合系统，交换机只负责聚合。
<ul>
<li>Count Sketch (CS): an offline algorithm, that needs an index table and aggregated results. Not implemented in Tofino yet. Can be used in the DP phase.</li>
</ul>
</li>
</ul>

  
  
    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
  

  <p><a class="classtest-link" href="/tags/Distributed-Training/" rel="tag">Distributed Training</a>, <a class="classtest-link" href="/tags/LLM-Training/" rel="tag">LLM Training</a>, <a class="classtest-link" href="/tags/Sparse-Model/" rel="tag">Sparse Model</a> — Mar 26, 2024</p>

          
          
        </div>
      </div>

    </div>

  </div>
  <script src="/js/nanobar.min.js"></script>
  <script>
    var options = {
      classname: 'nanobar',
      id: 'myNanobar'
    };
    var nanobar = new Nanobar(options);
    nanobar.go(30);
    nanobar.go(76);
    nanobar.go(100);
  </script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>

</html>
