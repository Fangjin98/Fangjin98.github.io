<!DOCTYPE html>
<html lang="en">

<head>

  <!-- Minima -->
  <!-- Hexo theme created by @adisaktijrs -->

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">

  
  <title>Distributed Training Research</title>
  
  <link rel="sitemap" href="https://Fangjin98.github.iositemap.xml" />
  
  <link rel="canonical" href="https://fangjin98.github.io/2024/03/18/dt/">
  
  <meta name="description" content="LLM Training  NSDI 24, MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs ZeRO++: Extremely Efficient Collective Communication ">
  
  
  <meta name="keywords" content="CS, Computer Networks, Powerlifting">
  
  <meta name="author" content="Joey">
  
  <meta property="og:image" content="https://Fangjin98.github.ioundefined">
  
  <meta property="og:site_name" content="Joey&#39;s Blog" />
  <meta property="og:type" content="article" />
  <meta property="og:title" content="Distributed Training Research" />
  
  <meta property="og:description" content="LLM Training  NSDI 24, MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs ZeRO++: Extremely Efficient Collective Communication ">
  
  <meta property="og:url" content="https://fangjin98.github.io/2024/03/18/dt/" />

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Distributed Training Research">
  
  <meta name="twitter:description" content="LLM Training  NSDI 24, MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs ZeRO++: Extremely Efficient Collective Communication ">
  
  
  <meta name="twitter:image" content="https://Fangjin98.github.ioundefined">
  
  <meta name="twitter:url" content="https://fangjin98.github.io/2024/03/18/dt/" />

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Preload fonts
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="preload" href="/fonts/dm-serif-display-v4-latin-regular.woff2" as="font" type="font/woff2" crossorigin>
  <link rel="preload" href="/fonts/inter-v2-latin-regular.woff2" as="font" type="font/woff2" crossorigin>

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  
<link rel="stylesheet" href="/css/normalize.css">

  
<link rel="stylesheet" href="/css/skeleton.css">

  
<link rel="stylesheet" href="/css/custom.css">

  
<link rel="stylesheet" href="/css/prism-dark.css">

  
<link rel="stylesheet" href="/css/prism-line-numbers.css">

  <!-- User css -->
  
  
<link rel="stylesheet" href="/css/user.css">

  

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="/images/favicon.png">

  <!-- Custom Theme Color Style
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <style>
  a:not(.icon) {
    text-decoration-color: #0FA0CE;
    background-image: linear-gradient(
      to bottom,
      rgba(0, 0, 0, 0) 50%,
      #0FA0CE 50%
    );
  }
  blockquote {
    border-left: 8px solid #0FA0CE;
  }
  .nanobar .bar {
    background: #0FA0CE;
  }
  .button.button-primary:hover,
  button.button-primary:hover,
  input[type="submit"].button-primary:hover,
  input[type="reset"].button-primary:hover,
  input[type="button"].button-primary:hover,
  .button.button-primary:focus,
  button.button-primary:focus,
  input[type="submit"].button-primary:focus,
  input[type="reset"].button-primary:focus,
  input[type="button"].button-primary:focus {
    background-color: #0FA0CE;
    border-color: #0FA0CE;
  }
  input[type="email"]:focus,
  input[type="number"]:focus,
  input[type="search"]:focus,
  input[type="text"]:focus,
  input[type="tel"]:focus,
  input[type="url"]:focus,
  input[type="password"]:focus,
  textarea:focus,
  select:focus {
    border: 1px solid #0FA0CE;
  }
</style>

  <!-- Google Analytics (With Privacy Settings On)
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  

<meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Joey's Blog" type="application/atom+xml">
</head>

<body>
  <div class="container">
    <div class="row">
      <div>
        <div class="row">
  <div class="two columns" style="max-width: 50px">
    <h1 class="mt-2 mode">
      <div onclick=setDarkMode(true) id="darkBtn">☀️</div>
      <div onclick=setDarkMode(false) id="lightBtn" class=hidden>🌑</div>
      <script >
        if (localStorage.getItem('preferredTheme') == 'dark') {
          setDarkMode(true)
        }
        function setDarkMode(isDark) {
          var darkBtn = document.getElementById('darkBtn')
          var lightBtn = document.getElementById('lightBtn')
          if (isDark) {
            lightBtn.style.display = "block"
            darkBtn.style.display = "none"
            localStorage.setItem('preferredTheme', 'dark');
          } else {
            lightBtn.style.display = "none"
            darkBtn.style.display = "block"
            localStorage.removeItem('preferredTheme');
          }
          document.body.classList.toggle("darkmode");
        }
      </script>
    </h1>
  </div>

  <div class="six columns ml-1">
    <h1 class="mt-2">
      Hello World.
    </h1>
  </div>

  <div class="twelve columns">
    <div class="row">
      <div class="nine columns left">
        <a href="/">Home</a>
        
          
          <a href="/about" class="ml">About</a>
          
        
          
          <a href="/archives" class="ml">Posts</a>
          
        
          
          <a href="/pdf/resume.pdf" class="ml">CV</a>
          
        
        
          
            <a href="mailto:Fangjin98@outlook.com" target="_blank" class="ml">Email</a>
          
        
      </div>
    </div>
    <hr style="margin-bottom: 2.6rem">
  </div>
</div>

        <div class="trans">
            <h2>Distributed Training Research</h2>

  <h3 id="llm-training">LLM Training</h3>
<ul>
<li>NSDI 24, MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs</li>
<li>ZeRO++: Extremely Efficient Collective Communication for Giant Model Training</li>
<li>DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2110.14883.pdf">Colossal-AI A Unified Deep Learning System For Large-Scale Parallel Training</a></li>
<li><strong>Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</strong></li>
<li>GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism</li>
</ul>
<h3 id="gradient-compression">Gradient Compression</h3>
<ul>
<li>ATC 22, Campo: Cost-Aware Performance Optimization for Mixed-Precision Neural Network Training</li>
<li>Rethinking Floating Point Overheads for Mixed Precision DNN Accelerators</li>
<li>SIGCOMM 21, <a target="_blank" rel="noopener" href="https://conferences.sigcomm.org/sigcomm/2021/files/papers/3452296.3472904.pdf">Efficient Sparse Collective Communication and its application to Accelerate Distributed Deep Learning</a>, Sparsity, Communication</li>
<li>SOSP 21, Gradient Compression Supercharged High-Performance Data Parallel DNN Training,  Quantization, Collective Communication</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2402.07529.pdf">Accelerating Distributed Deep Learning using Lossless Homomorphic Compression</a></li>
<li>NSDI 24, <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2302.08545.pdf">THC: Accelerating Distributed Deep Learning Using Tensor Homomorphic Compression</a></li>
</ul>
<h3 id="model-parallelism">Model Parallelism</h3>
<ul>
<li>PipeDream: Generalized Pipeline Parallelism for DNN Training</li>
<li>OSDI’23, AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving</li>
<li>OSDI’22, Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning</li>
<li>INFOCOM’22, Efficient Pipeline Planning for Expedited Distributed DNN Training</li>
<li>HPDC’22, <a target="_blank" rel="noopener" href="https://dl.acm.org/doi/pdf/10.1145/3502181.3531462">Hare: Exploiting Inter-job and Intra-job Parallelism of Distributed Machine Learning on Heterogeneous GPUs</a></li>
</ul>
<h3 id="in-network-aggregation">In-network Aggregation</h3>
<blockquote>
<p>See this <a href="/2023/07/03/ina/">post</a> for details.</p>
</blockquote>
<h3 id="asynchronous-training">Asynchronous Training</h3>
<ul>
<li>Developing a Loss Prediction-based Asynchronous Stochastic Gradient Descent Algorithm for Distributed Training of Deep Neural Networks：这篇文章介绍了异步更新的模式。看来参数服务器并没有一个具体的阈值确定何时进行异步更新。这篇文章提出一个算法以弥补梯度损失值。</li>
<li>ICDCS 19, Dynamic Stale Synchronous Parallel Distributed Training for Deep Learning：提出一种自适应确定延迟轮数方法。</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1511.05950.pdf">Staleness-aware Async-SGD for Distributed Deep Learning</a>: 针对异步分布式模型训练调整学习率。</li>
<li><strong>Pathways: Asynchronous Distributed Dataflow for ML</strong>: PATHWAYS makes use of a novel asynchronous distributed dataﬂow design that lets the control plane execute in parallel despite dependencies in the data plane</li>
<li><a href=":/85780103fdc6433fa3bf60bd545cc777">Accelerating Distributed Reinforcement Learning with In-Switch Computing</a>：这篇文章提到了使用可编程交换机优化异步训练，理论上来说，我们是可以转移到深度神经网络场景的。这篇文章仅仅通过显性设置延迟边界保证收敛。</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2211.05739.pdf">FedLesScan: Mitigating Stragglers in Serverless Federated Learning</a></li>
<li><strong>Communication-Efficient Federated Deep Learning With Layerwise Asynchronous Model Update and Temporally Weighted Aggregation</strong>：模型不同层的更新频率不一样，一般来说，shallow 层的参数比 deep 层的参数更新更加频繁，因此本文提出根据不同层的更新频率异步训练。本文还提出一个加权聚合策略，从而利用之前聚合的本地模型。</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1705.09786.pdf">AMPNet: Asynchronous Model-Parallel Training for Dynamic Neural Networks</a></li>
<li>INT Lab, FedSA: A Semi-Asynchronous Federated Learning Mechanism in Heterogeneous Edge Computing：在联邦学习场景中，由于节点异构，数据分布以及网络资源等问题，同步学习需要忍受很大的同步代价。因此现有研究关注异步学习。这篇论文确定每轮训练中，参数服务器收到哪<em>k</em>个工作节点的梯度才更新全局模型。这篇论文根据<em>边缘异构性</em>和<em>数据分布</em>决定<em>k</em>的值。</li>
<li>INT Lab, Adaptive Asynchronous Federated Learning in Resource-Constrained Edge Computing：这篇文章根据实时系统状态，例如网络资源，为每一轮异步训练确定聚合的工作节点比例。和上一篇工作内容相似。这里是根据全局信息确定的M，感觉参考意义不大</li>
</ul>
<h4 id="straggler-problem">Straggler Problem</h4>
<ul>
<li>Nurd: Negative-Unlabeled Learning for Online Datacenter Straggler Prediction：这篇文章并不是分布式模型训练场景，而是分布式系统中。提出了一个利用无监督学习预测数据中心慢节点的方法。</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9488815"><strong>Live Gradient Compensation for Evading Stragglers in Distributed Learning</strong></a>：清华深研院发表于 INFOCOM 21。通过梯度补偿机制，解决较慢工作节点拖累训练速度的问题，并能保证训练快速收敛</li>
<li><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v70/tandon17a/tandon17a.pdf">Gradient Coding: Avoiding Stragglers in Distributed Learning</a>：在同步场景下通过复制数据和编码梯度，容忍一部分错误和慢节点</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1806.02508v1.pdf">Fast Distributed Deep Learning via Worker-adaptive Batch Sizing</a>：根据节点处理容量分配数据批大小，使较慢的节点处理更少的数据（这种方式的问题在于，完全认为慢节点是由于计算容量限制造成的）</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2102.06280.pdf"><strong>Straggler-Resilient Distributed Machine Learning with Dynamic Backup Workers</strong></a>：发表在 Arxiv 上。为了解决慢节点问题，一个方式是中心节点只聚合一部分工作节点的梯度，未被聚合的工作节点称为 Backup Workers。
<ul>
<li>通过一个分布式算法确定每个工作节点的备份工作节点数</li>
<li>实验数据集有点小</li>
<li>有收敛性分析</li>
</ul>
</li>
</ul>
<h3 id="gpu-scheduling">GPU Scheduling</h3>
<ul>
<li>Gandiva: Introspective Cluster Scheduling for Deep Learning</li>
<li>TON’23, Deep Learning-Based Job Placement in Distributed Machine Learning Clusters With Heterogeneous Workloads</li>
<li>SOCC’22, <a target="_blank" rel="noopener" href="https://dl.acm.org/doi/pdf/10.1145/3620678.3624669">Anticipatory Resource Allocation for ML Training</a></li>
<li><a target="_blank" rel="noopener" href="https://cn287zbjkb.feishu.cn/docx/XWdhdYaxDoncKBxlJBhcbL3Xnwd">Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning</a>：基于<strong>模型超参</strong>来优化放置任务
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/442196995">https://zhuanlan.zhihu.com/p/442196995</a></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://www.usenix.org/system/files/nsdi20-paper-mahajan.pdf">THEMIS: Fair and Efficient GPU Cluster Scheduling</a>： <strong>考虑任务公平性</strong>的GPU资源分配机制。</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2308.00852.pdf">CASSINI: Network-Aware Job Scheduling in Machine Learning Clusters</a>：结合ML训练任务流量特点的带宽分配机制。通过调整任务开始时间，使带宽错峰分配。</li>
<li>Tessel: Boosting Distributed Execution of Large DNN Models via Flexible Schedule Search</li>
</ul>
<h4 id="resource-fragmentation problem">Resource Fragmentation Problem</h4>
<ul>
<li>ATC 23, <a target="_blank" rel="noopener" href="https://cse.hkust.edu.hk/~weiwa/papers/fgd-atc23.pdf">Beware of Fragmentation Scheduling GPU-Sharing Workloads with Fragmentation Gradient Descent</a></li>
</ul>

  
  
    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
  

  <p><a class="classtest-link" href="/tags/Summary/" rel="tag">Summary</a> — Mar 18, 2024</p>

          
          
        </div>
      </div>

    </div>

  </div>
  <script src="/js/nanobar.min.js"></script>
  <script>
    var options = {
      classname: 'nanobar',
      id: 'myNanobar'
    };
    var nanobar = new Nanobar(options);
    nanobar.go(30);
    nanobar.go(76);
    nanobar.go(100);
  </script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>

</html>
