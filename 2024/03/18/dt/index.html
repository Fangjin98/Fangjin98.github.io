<!DOCTYPE html>
<html lang="en">

<head>

  <!-- Minima -->
  <!-- Hexo theme created by @adisaktijrs -->

  <!-- Basic Page Needs
  â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“ -->
  <meta charset="utf-8">

  
  <title>Distributed Training Research</title>
  
  <link rel="sitemap" href="https://Fangjin98.github.iositemap.xml" />
  
  <link rel="canonical" href="https://fangjin98.github.io/2024/03/18/dt/">
  
  <meta name="description" content="LLM Training  NSDI 24, MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs ZeRO++: Extremely Efficient Collective Communication ">
  
  
  <meta name="keywords" content="CS, Computer Networks, Powerlifting">
  
  <meta name="author" content="Joey">
  
  <meta property="og:image" content="https://Fangjin98.github.ioundefined">
  
  <meta property="og:site_name" content="Joey&#39;s Blog" />
  <meta property="og:type" content="article" />
  <meta property="og:title" content="Distributed Training Research" />
  
  <meta property="og:description" content="LLM Training  NSDI 24, MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs ZeRO++: Extremely Efficient Collective Communication ">
  
  <meta property="og:url" content="https://fangjin98.github.io/2024/03/18/dt/" />

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Distributed Training Research">
  
  <meta name="twitter:description" content="LLM Training  NSDI 24, MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs ZeRO++: Extremely Efficient Collective Communication ">
  
  
  <meta name="twitter:image" content="https://Fangjin98.github.ioundefined">
  
  <meta name="twitter:url" content="https://fangjin98.github.io/2024/03/18/dt/" />

  <!-- Mobile Specific Metas
  â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“ -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Preload fonts
  â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“ -->
  <link rel="preload" href="/fonts/dm-serif-display-v4-latin-regular.woff2" as="font" type="font/woff2" crossorigin>
  <link rel="preload" href="/fonts/inter-v2-latin-regular.woff2" as="font" type="font/woff2" crossorigin>

  <!-- CSS
  â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“ -->
  
<link rel="stylesheet" href="/css/normalize.css">

  
<link rel="stylesheet" href="/css/skeleton.css">

  
<link rel="stylesheet" href="/css/custom.css">

  
<link rel="stylesheet" href="/css/prism-dark.css">

  
<link rel="stylesheet" href="/css/prism-line-numbers.css">

  <!-- User css -->
  
  
<link rel="stylesheet" href="/css/user.css">

  

  <!-- Favicon
  â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“ -->
  <link rel="icon" type="image/png" href="/images/favicon.png">

  <!-- Custom Theme Color Style
  â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“ -->
  <style>
  a:not(.icon) {
    text-decoration-color: #0FA0CE;
    background-image: linear-gradient(
      to bottom,
      rgba(0, 0, 0, 0) 50%,
      #0FA0CE 50%
    );
  }
  blockquote {
    border-left: 8px solid #0FA0CE;
  }
  .nanobar .bar {
    background: #0FA0CE;
  }
  .button.button-primary:hover,
  button.button-primary:hover,
  input[type="submit"].button-primary:hover,
  input[type="reset"].button-primary:hover,
  input[type="button"].button-primary:hover,
  .button.button-primary:focus,
  button.button-primary:focus,
  input[type="submit"].button-primary:focus,
  input[type="reset"].button-primary:focus,
  input[type="button"].button-primary:focus {
    background-color: #0FA0CE;
    border-color: #0FA0CE;
  }
  input[type="email"]:focus,
  input[type="number"]:focus,
  input[type="search"]:focus,
  input[type="text"]:focus,
  input[type="tel"]:focus,
  input[type="url"]:focus,
  input[type="password"]:focus,
  textarea:focus,
  select:focus {
    border: 1px solid #0FA0CE;
  }
</style>

  <!-- Google Analytics (With Privacy Settings On)
  â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“ -->
  

<meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Joey's Blog" type="application/atom+xml">
</head>

<body>
  <div class="container">
    <div class="row">
      <div>
        <div class="row">
  <div class="two columns" style="max-width: 50px">
    <h1 class="mt-2 mode">
      <div onclick=setDarkMode(true) id="darkBtn">â˜€ï¸</div>
      <div onclick=setDarkMode(false) id="lightBtn" class=hidden>ğŸŒ‘</div>
      <script >
        if (localStorage.getItem('preferredTheme') == 'dark') {
          setDarkMode(true)
        }
        function setDarkMode(isDark) {
          var darkBtn = document.getElementById('darkBtn')
          var lightBtn = document.getElementById('lightBtn')
          if (isDark) {
            lightBtn.style.display = "block"
            darkBtn.style.display = "none"
            localStorage.setItem('preferredTheme', 'dark');
          } else {
            lightBtn.style.display = "none"
            darkBtn.style.display = "block"
            localStorage.removeItem('preferredTheme');
          }
          document.body.classList.toggle("darkmode");
        }
      </script>
    </h1>
  </div>

  <div class="six columns ml-1">
    <h1 class="mt-2">
      Hello World.
    </h1>
  </div>

  <div class="twelve columns">
    <div class="row">
      <div class="nine columns left">
        <a href="/">Home</a>
        
          
          <a href="/about" class="ml">About</a>
          
        
          
          <a href="/archives" class="ml">Posts</a>
          
        
          
          <a href="/pdf/resume.pdf" class="ml">CV</a>
          
        
        
          
            <a href="mailto:Fangjin98@outlook.com" target="_blank" class="ml">Email</a>
          
        
      </div>
    </div>
    <hr style="margin-bottom: 2.6rem">
  </div>
</div>

        <div class="trans">
            <h2>Distributed Training Research</h2>

  <h3 id="llm-training">LLM Training</h3>
<ul>
<li>NSDI 24, MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs</li>
<li>ZeRO++: Extremely Efficient Collective Communication for Giant Model Training</li>
<li>DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2110.14883.pdf">Colossal-AI A Unified Deep Learning System For Large-Scale Parallel Training</a></li>
<li><strong>Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</strong></li>
<li>GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism</li>
</ul>
<h3 id="gradient-compression">Gradient Compression</h3>
<ul>
<li>ATC 22, Campo: Cost-Aware Performance Optimization for Mixed-Precision Neural Network Training</li>
<li>Rethinking Floating Point Overheads for Mixed Precision DNN Accelerators</li>
<li>SIGCOMM 21, <a target="_blank" rel="noopener" href="https://conferences.sigcomm.org/sigcomm/2021/files/papers/3452296.3472904.pdf">Efficient Sparse Collective Communication and its application to Accelerate Distributed Deep Learning</a>, Sparsity, Communication</li>
<li>SOSP 21, Gradient Compression Supercharged High-Performance Data Parallel DNN Training,  Quantization, Collective Communication</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2402.07529.pdf">Accelerating Distributed Deep Learning using Lossless Homomorphic Compression</a></li>
<li>NSDI 24, <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2302.08545.pdf">THC: Accelerating Distributed Deep Learning Using Tensor Homomorphic Compression</a></li>
</ul>
<h3 id="model-parallelism">Model Parallelism</h3>
<ul>
<li>PipeDream: Generalized Pipeline Parallelism for DNN Training</li>
<li>OSDIâ€™23, AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving</li>
<li>OSDIâ€™22, Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning</li>
<li>INFOCOMâ€™22, Efficient Pipeline Planning for Expedited Distributed DNN Training</li>
<li>HPDCâ€™22, <a target="_blank" rel="noopener" href="https://dl.acm.org/doi/pdf/10.1145/3502181.3531462">Hare: Exploiting Inter-job and Intra-job Parallelism of Distributed Machine Learning on Heterogeneous GPUs</a></li>
</ul>
<h3 id="in-network-aggregation">In-network Aggregation</h3>
<blockquote>
<p>See this <a href="/2023/07/03/ina/">post</a> for details.</p>
</blockquote>
<h3 id="asynchronous-training">Asynchronous Training</h3>
<ul>
<li>Developing a Loss Prediction-based Asynchronous Stochastic Gradient Descent Algorithm for Distributed Training of Deep Neural Networksï¼šè¿™ç¯‡æ–‡ç« ä»‹ç»äº†å¼‚æ­¥æ›´æ–°çš„æ¨¡å¼ã€‚çœ‹æ¥å‚æ•°æœåŠ¡å™¨å¹¶æ²¡æœ‰ä¸€ä¸ªå…·ä½“çš„é˜ˆå€¼ç¡®å®šä½•æ—¶è¿›è¡Œå¼‚æ­¥æ›´æ–°ã€‚è¿™ç¯‡æ–‡ç« æå‡ºä¸€ä¸ªç®—æ³•ä»¥å¼¥è¡¥æ¢¯åº¦æŸå¤±å€¼ã€‚</li>
<li>ICDCS 19, Dynamic Stale Synchronous Parallel Distributed Training for Deep Learningï¼šæå‡ºä¸€ç§è‡ªé€‚åº”ç¡®å®šå»¶è¿Ÿè½®æ•°æ–¹æ³•ã€‚</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1511.05950.pdf">Staleness-aware Async-SGD for Distributed Deep Learning</a>: é’ˆå¯¹å¼‚æ­¥åˆ†å¸ƒå¼æ¨¡å‹è®­ç»ƒè°ƒæ•´å­¦ä¹ ç‡ã€‚</li>
<li><strong>Pathways: Asynchronous Distributed Dataflow for ML</strong>: PATHWAYS makes use of a novel asynchronous distributed dataï¬‚ow design that lets the control plane execute in parallel despite dependencies in the data plane</li>
<li><a href=":/85780103fdc6433fa3bf60bd545cc777">Accelerating Distributed Reinforcement Learning with In-Switch Computing</a>ï¼šè¿™ç¯‡æ–‡ç« æåˆ°äº†ä½¿ç”¨å¯ç¼–ç¨‹äº¤æ¢æœºä¼˜åŒ–å¼‚æ­¥è®­ç»ƒï¼Œç†è®ºä¸Šæ¥è¯´ï¼Œæˆ‘ä»¬æ˜¯å¯ä»¥è½¬ç§»åˆ°æ·±åº¦ç¥ç»ç½‘ç»œåœºæ™¯çš„ã€‚è¿™ç¯‡æ–‡ç« ä»…ä»…é€šè¿‡æ˜¾æ€§è®¾ç½®å»¶è¿Ÿè¾¹ç•Œä¿è¯æ”¶æ•›ã€‚</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2211.05739.pdf">FedLesScan: Mitigating Stragglers in Serverless Federated Learning</a></li>
<li><strong>Communication-Efficient Federated Deep Learning With Layerwise Asynchronous Model Update and Temporally Weighted Aggregation</strong>ï¼šæ¨¡å‹ä¸åŒå±‚çš„æ›´æ–°é¢‘ç‡ä¸ä¸€æ ·ï¼Œä¸€èˆ¬æ¥è¯´ï¼Œshallow å±‚çš„å‚æ•°æ¯” deep å±‚çš„å‚æ•°æ›´æ–°æ›´åŠ é¢‘ç¹ï¼Œå› æ­¤æœ¬æ–‡æå‡ºæ ¹æ®ä¸åŒå±‚çš„æ›´æ–°é¢‘ç‡å¼‚æ­¥è®­ç»ƒã€‚æœ¬æ–‡è¿˜æå‡ºä¸€ä¸ªåŠ æƒèšåˆç­–ç•¥ï¼Œä»è€Œåˆ©ç”¨ä¹‹å‰èšåˆçš„æœ¬åœ°æ¨¡å‹ã€‚</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1705.09786.pdf">AMPNet: Asynchronous Model-Parallel Training for Dynamic Neural Networks</a></li>
<li>INT Lab, FedSA: A Semi-Asynchronous Federated Learning Mechanism in Heterogeneous Edge Computingï¼šåœ¨è”é‚¦å­¦ä¹ åœºæ™¯ä¸­ï¼Œç”±äºèŠ‚ç‚¹å¼‚æ„ï¼Œæ•°æ®åˆ†å¸ƒä»¥åŠç½‘ç»œèµ„æºç­‰é—®é¢˜ï¼ŒåŒæ­¥å­¦ä¹ éœ€è¦å¿å—å¾ˆå¤§çš„åŒæ­¥ä»£ä»·ã€‚å› æ­¤ç°æœ‰ç ”ç©¶å…³æ³¨å¼‚æ­¥å­¦ä¹ ã€‚è¿™ç¯‡è®ºæ–‡ç¡®å®šæ¯è½®è®­ç»ƒä¸­ï¼Œå‚æ•°æœåŠ¡å™¨æ”¶åˆ°å“ª<em>k</em>ä¸ªå·¥ä½œèŠ‚ç‚¹çš„æ¢¯åº¦æ‰æ›´æ–°å…¨å±€æ¨¡å‹ã€‚è¿™ç¯‡è®ºæ–‡æ ¹æ®<em>è¾¹ç¼˜å¼‚æ„æ€§</em>å’Œ<em>æ•°æ®åˆ†å¸ƒ</em>å†³å®š<em>k</em>çš„å€¼ã€‚</li>
<li>INT Lab, Adaptive Asynchronous Federated Learning in Resource-Constrained Edge Computingï¼šè¿™ç¯‡æ–‡ç« æ ¹æ®å®æ—¶ç³»ç»ŸçŠ¶æ€ï¼Œä¾‹å¦‚ç½‘ç»œèµ„æºï¼Œä¸ºæ¯ä¸€è½®å¼‚æ­¥è®­ç»ƒç¡®å®šèšåˆçš„å·¥ä½œèŠ‚ç‚¹æ¯”ä¾‹ã€‚å’Œä¸Šä¸€ç¯‡å·¥ä½œå†…å®¹ç›¸ä¼¼ã€‚è¿™é‡Œæ˜¯æ ¹æ®å…¨å±€ä¿¡æ¯ç¡®å®šçš„Mï¼Œæ„Ÿè§‰å‚è€ƒæ„ä¹‰ä¸å¤§</li>
</ul>
<h4 id="straggler-problem">Straggler Problem</h4>
<ul>
<li>Nurd: Negative-Unlabeled Learning for Online Datacenter Straggler Predictionï¼šè¿™ç¯‡æ–‡ç« å¹¶ä¸æ˜¯åˆ†å¸ƒå¼æ¨¡å‹è®­ç»ƒåœºæ™¯ï¼Œè€Œæ˜¯åˆ†å¸ƒå¼ç³»ç»Ÿä¸­ã€‚æå‡ºäº†ä¸€ä¸ªåˆ©ç”¨æ— ç›‘ç£å­¦ä¹ é¢„æµ‹æ•°æ®ä¸­å¿ƒæ…¢èŠ‚ç‚¹çš„æ–¹æ³•ã€‚</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9488815"><strong>Live Gradient Compensation for Evading Stragglers in Distributed Learning</strong></a>ï¼šæ¸…åæ·±ç ”é™¢å‘è¡¨äº INFOCOM 21ã€‚é€šè¿‡æ¢¯åº¦è¡¥å¿æœºåˆ¶ï¼Œè§£å†³è¾ƒæ…¢å·¥ä½œèŠ‚ç‚¹æ‹–ç´¯è®­ç»ƒé€Ÿåº¦çš„é—®é¢˜ï¼Œå¹¶èƒ½ä¿è¯è®­ç»ƒå¿«é€Ÿæ”¶æ•›</li>
<li><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v70/tandon17a/tandon17a.pdf">Gradient Coding: Avoiding Stragglers in Distributed Learning</a>ï¼šåœ¨åŒæ­¥åœºæ™¯ä¸‹é€šè¿‡å¤åˆ¶æ•°æ®å’Œç¼–ç æ¢¯åº¦ï¼Œå®¹å¿ä¸€éƒ¨åˆ†é”™è¯¯å’Œæ…¢èŠ‚ç‚¹</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1806.02508v1.pdf">Fast Distributed Deep Learning via Worker-adaptive Batch Sizing</a>ï¼šæ ¹æ®èŠ‚ç‚¹å¤„ç†å®¹é‡åˆ†é…æ•°æ®æ‰¹å¤§å°ï¼Œä½¿è¾ƒæ…¢çš„èŠ‚ç‚¹å¤„ç†æ›´å°‘çš„æ•°æ®ï¼ˆè¿™ç§æ–¹å¼çš„é—®é¢˜åœ¨äºï¼Œå®Œå…¨è®¤ä¸ºæ…¢èŠ‚ç‚¹æ˜¯ç”±äºè®¡ç®—å®¹é‡é™åˆ¶é€ æˆçš„ï¼‰</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2102.06280.pdf"><strong>Straggler-Resilient Distributed Machine Learning with Dynamic Backup Workers</strong></a>ï¼šå‘è¡¨åœ¨ Arxiv ä¸Šã€‚ä¸ºäº†è§£å†³æ…¢èŠ‚ç‚¹é—®é¢˜ï¼Œä¸€ä¸ªæ–¹å¼æ˜¯ä¸­å¿ƒèŠ‚ç‚¹åªèšåˆä¸€éƒ¨åˆ†å·¥ä½œèŠ‚ç‚¹çš„æ¢¯åº¦ï¼Œæœªè¢«èšåˆçš„å·¥ä½œèŠ‚ç‚¹ç§°ä¸º Backup Workersã€‚
<ul>
<li>é€šè¿‡ä¸€ä¸ªåˆ†å¸ƒå¼ç®—æ³•ç¡®å®šæ¯ä¸ªå·¥ä½œèŠ‚ç‚¹çš„å¤‡ä»½å·¥ä½œèŠ‚ç‚¹æ•°</li>
<li>å®éªŒæ•°æ®é›†æœ‰ç‚¹å°</li>
<li>æœ‰æ”¶æ•›æ€§åˆ†æ</li>
</ul>
</li>
</ul>
<h3 id="gpu-scheduling">GPU Scheduling</h3>
<ul>
<li>Gandiva: Introspective Cluster Scheduling for Deep Learning</li>
<li>TONâ€™23, Deep Learning-Based Job Placement in Distributed Machine Learning Clusters With Heterogeneous Workloads</li>
<li>SOCCâ€™22, <a target="_blank" rel="noopener" href="https://dl.acm.org/doi/pdf/10.1145/3620678.3624669">Anticipatory Resource Allocation for ML Training</a></li>
<li><a target="_blank" rel="noopener" href="https://cn287zbjkb.feishu.cn/docx/XWdhdYaxDoncKBxlJBhcbL3Xnwd">Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning</a>ï¼šåŸºäº<strong>æ¨¡å‹è¶…å‚</strong>æ¥ä¼˜åŒ–æ”¾ç½®ä»»åŠ¡
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/442196995">https://zhuanlan.zhihu.com/p/442196995</a></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://www.usenix.org/system/files/nsdi20-paper-mahajan.pdf">THEMIS: Fair and Efficient GPU Cluster Scheduling</a>ï¼š <strong>è€ƒè™‘ä»»åŠ¡å…¬å¹³æ€§</strong>çš„GPUèµ„æºåˆ†é…æœºåˆ¶ã€‚</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2308.00852.pdf">CASSINI: Network-Aware Job Scheduling in Machine Learning Clusters</a>ï¼šç»“åˆMLè®­ç»ƒä»»åŠ¡æµé‡ç‰¹ç‚¹çš„å¸¦å®½åˆ†é…æœºåˆ¶ã€‚é€šè¿‡è°ƒæ•´ä»»åŠ¡å¼€å§‹æ—¶é—´ï¼Œä½¿å¸¦å®½é”™å³°åˆ†é…ã€‚</li>
<li>Tessel: Boosting Distributed Execution of Large DNN Models via Flexible Schedule Search</li>
</ul>
<h4 id="resource-fragmentation problem">Resource Fragmentation Problem</h4>
<ul>
<li>ATC 23, <a target="_blank" rel="noopener" href="https://cse.hkust.edu.hk/~weiwa/papers/fgd-atc23.pdf">Beware of Fragmentation Scheduling GPU-Sharing Workloads with Fragmentation Gradient Descent</a></li>
</ul>

  
  
    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
  

  <p><a class="classtest-link" href="/tags/Summary/" rel="tag">Summary</a> â€” Mar 18, 2024</p>

          
          
        </div>
      </div>

    </div>

  </div>
  <script src="/js/nanobar.min.js"></script>
  <script>
    var options = {
      classname: 'nanobar',
      id: 'myNanobar'
    };
    var nanobar = new Nanobar(options);
    nanobar.go(30);
    nanobar.go(76);
    nanobar.go(100);
  </script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>

</html>
